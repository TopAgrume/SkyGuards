version: '3.8'
services:

#  skyguards:
#    build:
#      context: ./skyguards
#    depends_on:
#      - broker
#    environment:
#      PL_KAFKA_HOST: kafka:9092
#      PL_REPORT_TOPIC: reports
#      PL_NB_PEACEWATCHER: 5
#    deploy:
#      replicas: 1

  #########
  # KAFKA #
  #########

  kafka-zookeeper-1:
    image: confluentinc/cp-zookeeper:7.6.1
    user: root
    restart: on-failure
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka-broker-1:
    image: confluentinc/cp-kafka:7.6.1
    restart: on-failure
    hostname: kafka-broker-1
    ports:
      - "29092:29092"
    depends_on:
      - kafka-zookeeper-1
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "kafka-zookeeper-1:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      TOPIC_AUTO_CREATE: false

  init-kafka-topic-1:
    image: confluentinc/cp-kafka:7.6.1
    volumes:
      - ./script:/script/
    environment:
      KAFKA_BROKER: "kafka-broker-1:9092"
    depends_on:
      - kafka-broker-1
    entrypoint: ["/bin/sh", "-c"]
    command: [ 'sh /script/setup-kafka.sh' ]

  #########
  # HDFS #
  #########

  hdfs:
    image: alpine:3.20.1
    depends_on:
      - namenode
      - datanode
      - resourcemanager
      - nodemanager1
      - historyserver

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    expose:
      - 9000
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  #########
  # SPARK #
  #########

  spark-master-hdfs:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master-hdfs
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"  # Spark UI
      - "7077:7077"  # Spark Master
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker-hdfs-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-hdfs-1
    depends_on:
      - spark-master-hdfs
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master-hdfs:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-master:
    user: root
    hostname: spark-master
    image: spark:3.5.1-scala2.12-java11-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_LOCAL_IP=spark-master
    depends_on:
      - kafka-broker-1
    ports:
      - "8082:8080"
      - "7087:7077"

  spark-worker-a:
    image: spark:3.5.1-scala2.12-java11-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    depends_on:
      - spark-master
    ports:
      - "9991:8080"
      - "7000:7000"

  spark-worker-b:
    image: spark:3.5.1-scala2.12-java11-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    depends_on:
      - spark-master
    ports:
      - "9992:8080"
      - "7001:7000"

  ##############
  # SLOW START #
  ##############

  kafka:
    image: tianon/true
    depends_on:
      - kafka-broker-1
      - init-kafka-topic-1
    restart: "no"

  spark:
    image: tianon/true
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    restart: "no"

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
