services:

  #############
  # SERVICES  #
  #############

  #skyguards-service:
  #  build:
  #    context: ./skyguards
  #  depends_on:
  #    - kafka-broker-1
  #  environment:
  #    KAFKA_HOST: kafka-broker-1:9092
  #    REPORT_TOPIC: reports
  #    SCENARIO: 1
  #    NB_DRONE: 5
  #  deploy:
  #    replicas: 1
  #  command: ["sbt", "run"]

  skyguards-spark-alerts:
    build: ./spark-streaming
    depends_on:
      init-kafka-topic-1:
        condition: service_healthy
      spark-master:
        condition: service_started
    networks:
      - spark-network
      - default


  #########
  # KAFKA #
  #########

  kafka-zookeeper-1:
    image: confluentinc/cp-zookeeper:7.6.1
    user: root
    restart: on-failure
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka-broker-1:
    image: confluentinc/cp-kafka:7.6.1
    restart: on-failure
    hostname: kafka-broker-1
    ports:
      - "29092:29092"
    depends_on:
      - kafka-zookeeper-1
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "kafka-zookeeper-1:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      TOPIC_AUTO_CREATE: false

  kafka-broker-2:
    image: confluentinc/cp-kafka:7.6.1
    restart: on-failure
    hostname: kafka-broker-2
    ports:
      - "29093:29093"
    depends_on:
      - kafka-zookeeper-1
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: "kafka-zookeeper-1:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-2:9092,PLAINTEXT_INTERNAL://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      TOPIC_AUTO_CREATE: false

  kafka-broker-3:
    image: confluentinc/cp-kafka:7.6.1
    restart: on-failure
    hostname: kafka-broker-3
    ports:
      - "29094:29094"
    depends_on:
      - kafka-zookeeper-1
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: "kafka-zookeeper-1:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-3:9092,PLAINTEXT_INTERNAL://localhost:29094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      TOPIC_AUTO_CREATE: false

  init-kafka-topic-1:
    image: confluentinc/cp-kafka:7.6.1
    volumes:
      - ./script:/script/
    environment:
      KAFKA_BROKER: "kafka-broker-1:9092"
    depends_on:
      - kafka-broker-1
    entrypoint: ["/bin/sh", "-c"]
    command: [ 'sh /script/setup-kafka.sh' ]
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka-broker-1:9092 --list | grep -q 'reports' && kafka-topics --bootstrap-server kafka-broker-1:9092 --list | grep -q 'alerts'"]
      interval: 4s
      timeout: 4s
      retries: 10

  ###################
  # SPARK STREAMING #
  ###################

  spark-master:
    user: root
    #hostname: spark-master
    image: apache/spark:3.5.1-scala2.12-java11-python3-r-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_LOCAL_IP=spark-master
    depends_on:
      - kafka-broker-1
    ports:
      - "9090:8080"
      - "7087:7077"
    networks:
      - spark-network

  spark-worker-a:
    image: apache/spark:3.5.1-scala2.12-java11-python3-r-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    depends_on:
      - spark-master
    ports:
      - "9991:8080"
      - "7000:7000"
    networks:
      - spark-network

  spark-worker-b:
    image: apache/spark:3.5.1-scala2.12-java11-python3-r-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    depends_on:
      - spark-master
    ports:
      - "9992:8080"
      - "7001:7000"
    networks:
      - spark-network

  ############
  # ON ALERT #
  ############

  discord_bot:
    build: ./alert_consumers/discord
    command: python discord_bot.py
    depends_on:
      - init-kafka-topic-1
    volumes:
      - ./alert_consumers/discord:/app
    environment:
      - DISCORD_TOKEN=${DISCORD_TOKEN}
      - CHANNEL_ID_SECTOR_1=${CHANNEL_ID_SECTOR_1}
      - CHANNEL_ID_SECTOR_2=${CHANNEL_ID_SECTOR_2}
      - CHANNEL_ID_SECTOR_3=${CHANNEL_ID_SECTOR_3}
      - CHANNEL_ID_SECTOR_4=${CHANNEL_ID_SECTOR_4}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_DISCORD_GROUP=${KAFKA_DISCORD_GROUP}
    env_file:
      - .env

  telegram_bot:
    build: ./alert_consumers/telegram
    command: python telegram_bot.py
    depends_on:
      - init-kafka-topic-1
    volumes:
      - ./alert_consumers/telegram:/app
    environment:
      - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
      - CHAT_ID=${CHAT_ID}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_TELEGRAM_GROUP=${KAFKA_TELEGRAM_GROUP}
    env_file:
      - .env

  ########
  # HDFS #
  ########

  hdfs:
    build:
      context: ./hdfs
    depends_on:
      - namenode
      - datanode
      - resourcemanager
      - nodemanager1
      - historyserver

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    expose:
      - 9000
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://resourcemanager:8088 || exit 1" ]
      interval: 4s
      timeout: 4s
      retries: 10

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  ##############
  # SPARK HDFS #
  ##############

  spark-master-hdfs:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master-hdfs
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker-hdfs-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-hdfs-1
    depends_on:
      - spark-master-hdfs
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master-hdfs:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker-hdfs-2:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-hdfs-2
    depends_on:
      - spark-master-hdfs
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master-hdfs:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  ##############
  # SLOW START #
  ##############

  # Here we use tianon/true image:
  # This image essentially does nothing and exits immediately, which is useful for dependency management.

  #skyguards:
  #  image: tianon/true
  #  depends_on:
  #    - skyguards-service
  #  restart: "no"

  kafka:
    image: tianon/true
    depends_on:
      - kafka-broker-1
      - init-kafka-topic-1
    restart: "no"

  spark:
    image: tianon/true
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    restart: "no"

  spark-hdfs:
    image: tianon/true
    depends_on:
      - spark-master-hdfs
      - spark-worker-hdfs-1
      - spark-worker-hdfs-2
      - kafka
    restart: "no"

  alert-consumers:
    image: tianon/true
    depends_on:
      - discord_bot
      - telegram_bot
    restart: "no"

  analysis:
    build:
      context: ./analysis
    depends_on:
      - spark-master-hdfs
      - hdfs
    restart: "no"

  influxdb:
    image: influxdb:2.1.1
    volumes:
      - influxdb-storage:/var/lib/influxdb2:rw
    env_file:
      - .env
    entrypoint: [ "./entrypoint.sh" ]
    restart: on-failure:10
    ports:
      - ${DOCKER_INFLUXDB_INIT_PORT}:8086

  grafana:
    image: grafana/grafana-oss:8.4.3
    volumes:
      - grafana-storage:/var/lib/grafana:rw
    depends_on:
      - influxdb
    ports:
      - ${GRAFANA_PORT}:3000

  influx:
    build:
      ./influx
    depends_on:
      - grafana
      - kafka
      - spark
    restart: "no"
    environment:
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: "6cf082b94f7132a1487bc05729e7a3ec08db8b8d811bf8194508ed4b15d7c353"
      DOCKER_INFLUXDB_INIT_ORG: "skyguards"
      DOCKER_INFLUXDB_INIT_BUCKET: "website"
      DOCKER_INFLUXDB_INIT_PORT: 8086
      DOCKER_INFLUXDB_INIT_HOST: "influxdb"
    networks:
      - spark-network
      - default

networks:
  spark-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  grafana-storage:
  influxdb-storage:
