version: '3.8'
services:

  #############
  # SERVICES  #
  #############

  #skyguards-service:
  #  build:
  #    context: ./skyguards
  #  depends_on:
  #    - kafka-broker-1
  #  environment:
  #    KAFKA_HOST: kafka-broker-1:9092
  #    REPORT_TOPIC: reports
  #    SCENARIO: 1
  #    NB_DRONE: 5
  #  deploy:
  #    replicas: 1
  #  command: ["sbt", "run"]

  skyguards-spark-alerts:
    build: ./spark-streaming
    depends_on:
      init-kafka-topic-1:
        condition: service_healthy
      spark-master:
        condition: service_started
    networks:
      - spark-network
      - default


  #########
  # KAFKA #
  #########

  kafka-zookeeper-1:
    image: confluentinc/cp-zookeeper:7.6.1
    user: root
    restart: on-failure
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka-broker-1:
    image: confluentinc/cp-kafka:7.6.1
    restart: on-failure
    hostname: kafka-broker-1
    ports:
      - "29092:29092"
    depends_on:
      - kafka-zookeeper-1
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "kafka-zookeeper-1:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      TOPIC_AUTO_CREATE: false

  init-kafka-topic-1:
    image: confluentinc/cp-kafka:7.6.1
    volumes:
      - ./script:/script/
    environment:
      KAFKA_BROKER: "kafka-broker-1:9092"
    depends_on:
      - kafka-broker-1
    entrypoint: ["/bin/sh", "-c"]
    command: [ 'sh /script/setup-kafka.sh' ]
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka-broker-1:9092 --list | grep -q 'reports' && kafka-topics --bootstrap-server kafka-broker-1:9092 --list | grep -q 'alerts'"]
      interval: 4s
      timeout: 4s
      retries: 10

  #########
  # HDFS #
  #########

  hdfs:
    image: alpine:3.20.1
    depends_on:
      - namenode
      - datanode
      - resourcemanager
      - nodemanager1
      - historyserver

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    expose:
      - 9000
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./hadoop.env

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env

  ##############
  # SPARK HDFS #
  ##############

  spark-master-hdfs:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master-hdfs
    depends_on:
      - namenode
      - datanode
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker-hdfs-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-hdfs-1
    depends_on:
      - spark-master-hdfs
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master-hdfs:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  spark-worker-hdfs-2:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-hdfs-2
    depends_on:
      - spark-master-hdfs
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master-hdfs:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000

  #########
  # SPARK #
  #########

  # Made with the help of:
  # https://dev.to/mvillarrealb/creating-a-spark-standalone-cluster-with-docker-and-docker-compose-2021-update-6l4

  spark-master:
    user: root
    #hostname: spark-master
    image: apache/spark:3.5.1-scala2.12-java11-python3-r-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_LOCAL_IP=spark-master
    depends_on:
      - kafka-broker-1
    ports:
      - "9090:8080"
      - "7087:7077"
    networks:
      - spark-network

  spark-worker-a:
    image: apache/spark:3.5.1-scala2.12-java11-python3-r-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    depends_on:
      - spark-master
    ports:
      - "9991:8080"
      - "7000:7000"
    networks:
      - spark-network

  spark-worker-b:
    image: apache/spark:3.5.1-scala2.12-java11-python3-r-ubuntu
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=256M
      - SPARK_DRIVER_MEMORY=256M
      - SPARK_EXECUTOR_MEMORY=256M
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    depends_on:
      - spark-master
    ports:
      - "9992:8080"
      - "7001:7000"
    networks:
      - spark-network

  #############
  # S3 Bucket #
  #############

  ##############
  # SLOW START #
  ##############

  kafka:
    image: tianon/true
    depends_on:
      - kafka-broker-1
      - init-kafka-topic-1
    restart: "no"

  #skyguards:
  #  image: tianon/true
  #  depends_on:
  #    - skyguards-service
  #  restart: "no"

  spark:
    image: tianon/true
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    restart: "no"

networks:
  spark-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
